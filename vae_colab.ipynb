{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git clone https://github.com/dmhd1/variational-autoencoder.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd variational-autoencoder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nomen\n",
    "!pip install torch torchvisio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flow\n",
    "import train_variational_autoencoder_pytorch\n",
    "import data\n",
    "import torch\n",
    "torch.cuda.is_available("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = yaml.safe_load(config)\n",
    "cfg = nomen.Config(dictionary)\n",
    "cfg.parse_args()\n",
    "\n",
    "cfg.data_dir = pathlib.Path.cwd() / cfg.data_dir\n",
    "cfg.train_dir = pathlib.Path.cwd() / cfg.train_dir\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cfg.use_gpu else \"cpu\")\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "random.seed(cfg.seed)\n",
    "\n",
    "model = Model(latent_size=cfg.latent_size, data_size=cfg.data_size)\n",
    "if cfg.variational == \"flow\":\n",
    "    variational = VariationalFlow(\n",
    "        latent_size=cfg.latent_size,\n",
    "        data_size=cfg.data_size,\n",
    "        flow_depth=cfg.flow_depth,\n",
    "    )\n",
    "elif cfg.variational == \"mean-field\":\n",
    "    variational = VariationalMeanField(\n",
    "        latent_size=cfg.latent_size, data_size=cfg.data_size\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Variational distribution not implemented: %s\" % cfg.variational\n",
    "    )\n",
    "\n",
    "model.to(device)\n",
    "variational.to(device)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(\n",
    "    list(model.parameters()) + list(variational.parameters()),\n",
    "    lr=cfg.learning_rate,\n",
    "    centered=True,\n",
    ")\n",
    "\n",
    "kwargs = {\"num_workers\": 4, \"pin_memory\": True} if cfg.use_gpu else {}\n",
    "train_data, valid_data, test_data = load_binary_mnist(cfg, **kwargs)\n",
    "\n",
    "best_valid_elbo = -np.inf\n",
    "num_no_improvement = 0\n",
    "\n",
    "for step, batch in enumerate(cycle(train_data)):\n",
    "    x = batch[0].to(device)\n",
    "    model.zero_grad()\n",
    "    variational.zero_grad()\n",
    "    z, log_q_z = variational(x, n_samples=1)\n",
    "    log_p_x_and_z = model(z, x)\n",
    "    # average over sample dimension\n",
    "    elbo = (log_p_x_and_z - log_q_z).mean(1)\n",
    "    # sum over batch dimension\n",
    "    loss = -elbo.sum(0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % cfg.log_interval == 0:\n",
    "        print(\n",
    "            f\"step:\\t{step}\\ttrain elbo: {elbo.detach().cpu().numpy().mean():.2f}\"\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            valid_elbo, valid_log_p_x = evaluate(\n",
    "                cfg.n_samples, model, variational, valid_data\n",
    "            )\n",
    "        print(\n",
    "            f\"step:\\t{step}\\t\\tvalid elbo: {valid_elbo:.2f}\\tvalid log p(x): {valid_log_p_x:.2f}\"\n",
    "        )\n",
    "        if valid_elbo > best_valid_elbo:\n",
    "            num_no_improvement = 0\n",
    "            best_valid_elbo = valid_elbo\n",
    "            states = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"variational\": variational.state_dict(),\n",
    "            }\n",
    "            torch.save(states, cfg.train_dir / \"best_state_dict\")\n",
    "        else:\n",
    "            num_no_improvement += 1\n",
    "\n",
    "        if num_no_improvement > cfg.early_stopping_interval:\n",
    "            checkpoint = torch.load(cfg.train_dir / \"best_state_dict\")\n",
    "            model.load_state_dict(checkpoint[\"model\"])\n",
    "            variational.load_state_dict(checkpoint[\"variational\"])\n",
    "            with torch.no_grad():\n",
    "                test_elbo, test_log_p_x = evaluate(\n",
    "                    cfg.n_samples, model, variational, test_data\n",
    "                )\n",
    "            print(\n",
    "                f\"step:\\t{step}\\t\\ttest elbo: {test_elbo:.2f}\\ttest log p(x): {test_log_p_x:.2f}\"\n",
    "            )\n",
    "            break\n",
    "dictionary = yaml.safe_load(config)\n",
    "cfg = nomen.Config(dictionary)\n",
    "cfg.parse_args()\n",
    "\n",
    "cfg.data_dir = pathlib.Path.cwd() / cfg.data_dir\n",
    "cfg.train_dir = pathlib.Path.cwd() / cfg.train_dir\n",
    "\n",
    "device = torch.device(\"cuda:0\" if cfg.use_gpu else \"cpu\")\n",
    "torch.manual_seed(cfg.seed)\n",
    "np.random.seed(cfg.seed)\n",
    "random.seed(cfg.seed)\n",
    "\n",
    "model = Model(latent_size=cfg.latent_size, data_size=cfg.data_size)\n",
    "if cfg.variational == \"flow\":\n",
    "    variational = VariationalFlow(\n",
    "        latent_size=cfg.latent_size,\n",
    "        data_size=cfg.data_size,\n",
    "        flow_depth=cfg.flow_depth,\n",
    "    )\n",
    "elif cfg.variational == \"mean-field\":\n",
    "    variational = VariationalMeanField(\n",
    "        latent_size=cfg.latent_size, data_size=cfg.data_size\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Variational distribution not implemented: %s\" % cfg.variational\n",
    "    )\n",
    "\n",
    "model.to(device)\n",
    "variational.to(device)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(\n",
    "    list(model.parameters()) + list(variational.parameters()),\n",
    "    lr=cfg.learning_rate,\n",
    "    centered=True,\n",
    ")\n",
    "\n",
    "kwargs = {\"num_workers\": 4, \"pin_memory\": True} if cfg.use_gpu else {}\n",
    "train_data, valid_data, test_data = load_binary_mnist(cfg, **kwargs)\n",
    "\n",
    "best_valid_elbo = -np.inf\n",
    "num_no_improvement = 0\n",
    "\n",
    "for step, batch in enumerate(cycle(train_data)):\n",
    "    x = batch[0].to(device)\n",
    "    model.zero_grad()\n",
    "    variational.zero_grad()\n",
    "    z, log_q_z = variational(x, n_samples=1)\n",
    "    log_p_x_and_z = model(z, x)\n",
    "    # average over sample dimension\n",
    "    elbo = (log_p_x_and_z - log_q_z).mean(1)\n",
    "    # sum over batch dimension\n",
    "    loss = -elbo.sum(0)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % cfg.log_interval == 0:\n",
    "        print(\n",
    "            f\"step:\\t{step}\\ttrain elbo: {elbo.detach().cpu().numpy().mean():.2f}\"\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            valid_elbo, valid_log_p_x = evaluate(\n",
    "                cfg.n_samples, model, variational, valid_data\n",
    "            )\n",
    "        print(\n",
    "            f\"step:\\t{step}\\t\\tvalid elbo: {valid_elbo:.2f}\\tvalid log p(x): {valid_log_p_x:.2f}\"\n",
    "        )\n",
    "        if valid_elbo > best_valid_elbo:\n",
    "            num_no_improvement = 0\n",
    "            best_valid_elbo = valid_elbo\n",
    "            states = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"variational\": variational.state_dict(),\n",
    "            }\n",
    "            torch.save(states, cfg.train_dir / \"best_state_dict\")\n",
    "        else:\n",
    "            num_no_improvement += 1\n",
    "\n",
    "        if num_no_improvement > cfg.early_stopping_interval:\n",
    "            checkpoint = torch.load(cfg.train_dir / \"best_state_dict\")\n",
    "            model.load_state_dict(checkpoint[\"model\"])\n",
    "            variational.load_state_dict(checkpoint[\"variational\"])\n",
    "            with torch.no_grad():\n",
    "                test_elbo, test_log_p_x = evaluate(\n",
    "                    cfg.n_samples, model, variational, test_data\n",
    "                )\n",
    "            print(\n",
    "                f\"step:\\t{step}\\t\\ttest elbo: {test_elbo:.2f}\\ttest log p(x): {test_log_p_x:.2f}\"\n",
    "            )\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mc",
   "language": "python",
   "name": "mc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
